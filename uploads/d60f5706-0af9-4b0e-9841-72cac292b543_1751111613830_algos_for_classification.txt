Improving accuracy beyond logistic regression and random forests can depend on the nature of your data. Here are some alternative algorithms and strategies that you can consider to achieve better accuracy:

---

### 1. **Gradient Boosting Algorithms**
These algorithms often outperform Logistic Regression and Random Forests in many scenarios.

- **XGBoost (Extreme Gradient Boosting)**:
  - Known for its speed and performance.
  - Handles missing data and categorical features efficiently.
  ```python
  from xgboost import XGBClassifier
  model = XGBClassifier()
  model.fit(X_train, y_train)
  y_pred = model.predict(X_test)
  ```

- **LightGBM (Light Gradient Boosting Machine)**:
  - Fast, efficient, and handles large datasets well.
  ```python
  from lightgbm import LGBMClassifier
  model = LGBMClassifier()
  model.fit(X_train, y_train)
  y_pred = model.predict(X_test)
  ```

- **CatBoost**:
  - Handles categorical features natively.
  ```python
  from catboost import CatBoostClassifier
  model = CatBoostClassifier(verbose=0)
  model.fit(X_train, y_train)
  y_pred = model.predict(X_test)
  ```

---

### 2. **Support Vector Machines (SVM)**
SVMs work well for text and image classification and are effective in high-dimensional spaces.

```python
from sklearn.svm import SVC
model = SVC(kernel='linear', C=1.0, probability=True)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

---

### 3. **Neural Networks**
Deep learning models like neural networks often outperform traditional machine learning models for large datasets.

- **MLPClassifier** (Multi-Layer Perceptron) in `sklearn`:
  ```python
  from sklearn.neural_network import MLPClassifier
  model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)
  model.fit(X_train, y_train)
  y_pred = model.predict(X_test)
  ```

- **Deep Learning Frameworks**:
  Use frameworks like TensorFlow or PyTorch to build more complex models, especially for unstructured data like text or images.

---

### 4. **K-Nearest Neighbors (KNN)**
KNN can perform well on smaller datasets but may not scale well for larger datasets.

```python
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

---

### 5. **Ensemble Methods**
Combine the predictions of multiple models to boost performance.

- **Stacking**:
  Combine Logistic Regression, Random Forest, and Gradient Boosting into a meta-model.
  ```python
  from sklearn.ensemble import StackingClassifier
  from sklearn.linear_model import LogisticRegression
  from sklearn.ensemble import RandomForestClassifier

  base_estimators = [
      ('lr', LogisticRegression()),
      ('rf', RandomForestClassifier()),
      ('gb', XGBClassifier())
  ]
  model = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression())
  model.fit(X_train, y_train)
  y_pred = model.predict(X_test)
  ```

- **Voting**:
  Combine predictions from different algorithms (e.g., Logistic Regression, SVM, Random Forest) using majority voting.
  ```python
  from sklearn.ensemble import VotingClassifier

  model = VotingClassifier(estimators=[
      ('lr', LogisticRegression()),
      ('rf', RandomForestClassifier()),
      ('svm', SVC(probability=True))
  ], voting='soft')
  model.fit(X_train, y_train)
  y_pred = model.predict(X_test)
  ```

---

### 6. **Fine-Tuning and Feature Engineering**
Sometimes the problem isn't the model but the data.

- **Feature Engineering**:
  - Add domain-specific features or remove irrelevant ones.
  - Use polynomial or interaction features.
  
- **Hyperparameter Tuning**:
  Use grid search or random search to find optimal parameters.
  ```python
  from sklearn.model_selection import GridSearchCV

  param_grid = {'n_estimators': [100, 200], 'max_depth': [3, 5, 7]}
  grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
  grid.fit(X_train, y_train)
  best_model = grid.best_estimator_
  ```

---

### 7. **Additional Algorithms to Explore**
- **Logistic Regression with Regularization**:
  Try L1 (Lasso) or L2 (Ridge) regularization.
  
- **Naive Bayes Variants**:
  - **BernoulliNB** for binary features.
  - **ComplementNB** for imbalanced datasets.

- **Bagging**:
  Ensemble a weak learner like Decision Trees using bagging techniques.

---

If none of these work, consider revisiting your preprocessing, data augmentation, or normalizing/standardizing your data. Let me know the type of data you're working on (e.g., text, numeric, categorical) so I can suggest more targeted solutions!